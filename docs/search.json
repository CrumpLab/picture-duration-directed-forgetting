[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nothing to see here yet."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "picture-duration-directed-forgetting",
    "section": "",
    "text": "Currently under development\nThe goal of this project repository is to extend Patrick Ihejirika’s honor thesis project in a reproducible form using quarto, and to extend the thesis into a larger project with additional collaborators.\nIn theory this entire project is shared in a computationally reproducible format. The github repository for this website contains all of the source code, the original data, the data analysis, and the experiment scripts used to run the experiment."
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html",
    "href": "vignettes/E1_DF_Mixed.html",
    "title": "E1_DF_Mixed",
    "section": "",
    "text": "Data collected 2/10/22"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#import-data",
    "href": "vignettes/E1_DF_Mixed.html#import-data",
    "title": "E1_DF_Mixed",
    "section": "Import Data",
    "text": "Import Data\n\n# Read the text file from JATOS ...\nread_file('data/E1/jatos_results_20220330235250.txt') %>%\n  # ... split it into lines ...\n  str_split('\\n') %>% first() %>%\n  # ... filter empty rows ...\n  discard(function(x) x == '') %>%\n  # ... parse JSON into a data.frame\n  map_dfr(fromJSON, flatten=T) -> all_data"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#demographics",
    "href": "vignettes/E1_DF_Mixed.html#demographics",
    "title": "E1_DF_Mixed",
    "section": "Demographics",
    "text": "Demographics\n\nlibrary(tidyr)\n\ndemographics <- all_data %>%\n  filter(trial_type == \"survey-html-form\") %>%\n  select(ID,response) %>%\n  unnest_wider(response) %>%\n  mutate(age = as.numeric(age))\n\nage_demographics <- demographics %>%\n  summarize(mean_age = mean(age),\n            sd_age = sd(age),\n            min_age = min(age),\n            max_age = max(age))\n\nfactor_demographics <- apply(demographics[-1], 2, table)\n\nA total of 47 participants were recruited from Amazon’s Mechanical Turk. Mean age was 34.9 (range = 22 to 60 ). There were 23 females, and 24 males. There were 41 right-handed participants, and NA left or both handed participants. 37 participants reported normal vision, and 10 participants reported corrected-to-normal vision. 37 participants reported English as a first language, and 10 participants reported English as a second language."
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#pre-processing",
    "href": "vignettes/E1_DF_Mixed.html#pre-processing",
    "title": "E1_DF_Mixed",
    "section": "Pre-processing",
    "text": "Pre-processing\nWe are interested in including participants who attempted to perform the task to the best of their ability. We adopted the following exclusion criteria.\n\nLower than 75% correct during the encoding task. This means that participants failed to correctly press the F or R keys on each trial.\n\n\n# select data from the study phase\nstudy_accuracy <- all_data %>%\n  filter(experiment_phase == \"study\",\n         is.na(correct) == FALSE) %>%\n  group_by(ID)%>%\n  summarize(mean_correct = mean(correct))\n\nstudy_excluded_subjects <- study_accuracy %>%\n  filter(mean_correct < .75) %>%\n  pull(ID)\n\nggplot(study_accuracy, aes(x=mean_correct))+\n  coord_cartesian(xlim=c(0,1))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean correct responses \\n for each subject during study phase\")\n\n\n\n\n\nMore than 25% Null responses (120*.25 = 30) during test. NULL responses mean that the participant did not respond on a test trial after 10 seconds.\n\n\n# select data from the study phase\ntest_null <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response ==\"NULL\") %>%\n  group_by(ID) %>%\n  count()\n\ntest_null_excluded <- test_null %>%\n  filter(n > (120*.25)) %>%\n  pull(ID)\n\nggplot(test_null, aes(x=n))+\n  geom_vline(xintercept=30)+\n  geom_histogram()+\n  ggtitle(\"Histogram of count of null responses \\n for each subject during test\")\n\n\n\n\n\nHigher than 75% response bias in the recognition task. This suggests that participants were simply pressing the same button on most trials.\n\n\ntest_response_bias <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\") %>%\n  mutate(response = as.numeric(response)) %>%\n  group_by(ID, response) %>%\n  count() %>%\n  pivot_wider(names_from = response,\n              values_from = n,\n              values_fill = 0) %>%\n  mutate(bias = abs(`0` - `1`)/120)\n\ntest_response_bias_excluded <- test_response_bias %>%\n  filter(bias > .75) %>%\n  pull(ID)\n\nggplot(test_response_bias, aes(x=bias))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nMaking responses too fast during the recognition memory test, indicating that they weren’t performing the task. We excluded participants whose mean RT was less than 300 ms.\n\n\ntest_mean_rt <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\",\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt)) %>%\n  group_by(ID) %>%\n  summarize(mean_RT = mean(rt))\n\ntest_mean_rt_excluded <- test_mean_rt %>%\n  filter(mean_RT < 300) %>%\n  pull(ID)\n\nggplot(test_mean_rt, aes(x=mean_RT))+\n  geom_vline(xintercept=300)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nSubjects are included if they perform better than 55% correct on the novel lures.\n\n\ntest_mean_novel_accuracy <- all_data %>%\n  filter(experiment_phase == \"test\",\n         test_condition == \"novel\") %>%\n  mutate(correct = as.logical(correct)) %>%\n  group_by(ID) %>%\n  summarize(mean_correct = mean(correct))\n\ntest_mean_novel_accuracy_excluded <- test_mean_novel_accuracy %>%\n  filter(mean_correct < .55) %>%\n  pull(ID)\n\nggplot(test_mean_novel_accuracy, aes(x=mean_correct))+\n  geom_vline(xintercept=.55)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean accuracy for novel lures \\n for each subject during test phase\")"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#all-exclusions",
    "href": "vignettes/E1_DF_Mixed.html#all-exclusions",
    "title": "E1_DF_Mixed",
    "section": "All exclusions",
    "text": "All exclusions\n\nall_excluded <- unique(c(study_excluded_subjects,\n                  test_null_excluded,\n                  test_response_bias_excluded,\n                  test_mean_rt_excluded,\n                  test_mean_novel_accuracy_excluded))\n\nlength(all_excluded)\n\n[1] 13\n\n\nOur participants were recruited online and completed the experiment from a web browser. Our experiment script requests that participants attempt the task to the best of their ability. Nevertheless, it is possible that participants complete the experiment and submit data without attempting to complete the task as directed. We developed a set of criteria to exclude participants whose performance indicated they were not attempting the task as instructed. These criteria also allowed us to confirm that the participants we included in the analysis did attempt the task as instructed to the best of their ability. We adopted the following five criteria:\nFirst, during the encoding phase participants responded to each instructional cue (to remember or forget the picture on each trial) by pressing “R” or “F” on the keyboard. This task demand further served as an attentional check. We excluded participants who scored lower than 75% correct on instructional cue identification responses. Second, participants who did not respond on more than 25% of trials in the recognition test were excluded. Third, we measured response bias (choosing the left or right picture) during the recognition test, and excluded participants who made 75% of their responses to one side (indicating they were repeatedly pressing the same button on each trial). Fourth, we excluded participants whose mean reaction time during the recognition test was less than 300ms, indicating they were pressing the buttons as fast as possible without making a recognition decision. Finally, we computed mean accuracy for the novel lure condition for all participants, and excluded participants whose mean accuracy was less than 55% for those items. All together 13 participants were excluded."
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#define-helper-functions",
    "href": "vignettes/E1_DF_Mixed.html#define-helper-functions",
    "title": "E1_DF_Mixed",
    "section": "Define Helper functions",
    "text": "Define Helper functions\nTo do, consider moving the functions into the R package for this project\n\n# attempt general solution\n\n## Declare helper functions\n\n################\n# get_mean_sem\n# data = a data frame\n# grouping_vars = a character vector of factors for analysis contained in data\n# dv = a string indicated the dependent variable colunmn name in data\n# returns data frame with grouping variables, and mean_{dv}, sem_{dv}\n# note: dv in mean_{dv} and sem_{dv} is renamed to the string in dv\n\nget_mean_sem <- function(data, grouping_vars, dv, digits=3){\n  a <- data %>%\n    group_by_at(grouping_vars) %>%\n    summarize(\"mean_{ dv }\" := round(mean(.data[[dv]]), digits),\n              \"sem_{ dv }\" := round(sd(.data[[dv]])/sqrt(length(.data[[dv]])),digits),\n              .groups=\"drop\")\n  return(a)\n}\n\n################\n# get_effect_names\n# grouping_vars = a character vector of factors for analysis\n# returns a named list\n# list contains all main effects and interaction terms\n# useful for iterating the computation means across design effects and interactions\n\nget_effect_names <- function(grouping_vars){\n  effect_names <- grouping_vars\n  if( length(grouping_vars > 1) ){\n    for( i in 2:length(grouping_vars) ){\n      effect_names <- c(effect_names,apply(combn(grouping_vars,i),2,paste0,collapse=\":\"))\n    }\n  }\n  effects <- strsplit(effect_names, split=\":\")\n  names(effects) <- effect_names\n  return(effects)\n}\n\n################\n# print_list_of_tables\n# table_list = a list of named tables\n# each table is printed \n# names are header level 3\n\nprint_list_of_tables <- function(table_list){\n  for(i in 1:length(table_list)){\n    cat(\"###\",names(table_list[i]))\n    cat(\"\\n\")\n    print(knitr::kable(table_list[[i]]))\n    cat(\"\\n\")\n  }\n}"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#conduct-analysis",
    "href": "vignettes/E1_DF_Mixed.html#conduct-analysis",
    "title": "E1_DF_Mixed",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nAccuracy <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nAccuracy$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE)\n\n# declare factors, IVS, subject variable, and DV\nAccuracy$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nAccuracy$factors$subject <- \"ID\"\nAccuracy$factors$DV <- \"correct\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nAccuracy$subject_means <- get_mean_sem(data=Accuracy$filtered_data,\n                                       grouping_vars = c(Accuracy$factors$subject,\n                                                         Accuracy$factors$IVs),\n                                       dv = Accuracy$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nAccuracy$effects <- get_effect_names(Accuracy$factors$IVs)\n\nAccuracy$means <- lapply(Accuracy$effects, FUN = function(x) {\n  get_mean_sem(data=Accuracy$filtered_data,\n             grouping_vars = x,\n             dv = Accuracy$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nAccuracy$subject_means <- Accuracy$subject_means %>%\n  mutate_at(Accuracy$factors$IVs,factor) %>%\n  mutate_at(Accuracy$factors$subject,factor)\n\n# run ANOVA\nAccuracy$aov.out <- aov(mean_correct ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), Accuracy$subject_means)\n\n# save printable summaries\nAccuracy$apa_print <- papaja::apa_print(Accuracy$aov.out)"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#graphs",
    "href": "vignettes/E1_DF_Mixed.html#graphs",
    "title": "E1_DF_Mixed",
    "section": "Graphs",
    "text": "Graphs\n\nAccuracy$graphs$figure <- ggplot(Accuracy$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_correct,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_correct-sem_correct,\n                    ymax = mean_correct+sem_correct),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(.4,1))+\n  geom_hline(yintercept=.5)+\n  scale_y_continuous(breaks = seq(0.4,1,.1))+\n  theme_classic(base_size=12)+\n  ylab(\"Proportion Correct\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E1: Proportion Correct by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nAccuracy$graphs$figure"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#print-anova",
    "href": "vignettes/E1_DF_Mixed.html#print-anova",
    "title": "E1_DF_Mixed",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(Accuracy$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n33\n4.7925490\n0.1452288\nNA\nNA\n\n\nencoding_stimulus_time\n2\n0.0885784\n0.0442892\n2.6182656\n0.0804931\n\n\nResiduals\n66\n1.1164216\n0.0169155\nNA\nNA\n\n\nencoding_instruction\n1\n0.0283333\n0.0283333\n1.9547038\n0.1714078\n\n\nResiduals\n33\n0.4783333\n0.0144949\nNA\nNA\n\n\ntest_condition\n1\n4.1603922\n4.1603922\n133.3448867\n0.0000000\n\n\nResiduals\n33\n1.0296078\n0.0312002\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n0.0415196\n0.0207598\n0.9470100\n0.3931059\n\n\nResiduals\n66\n1.4468137\n0.0219214\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n0.0918137\n0.0459069\n2.9904204\n0.0571205\n\n\nResiduals\n66\n1.0131863\n0.0153513\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n0.0165686\n0.0165686\n1.2423702\n0.2730668\n\n\nResiduals\n33\n0.4400980\n0.0133363\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n0.0006373\n0.0003186\n0.0121719\n0.9879041\n\n\nResiduals\n66\n1.7276961\n0.0261772\nNA\nNA"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#print-means",
    "href": "vignettes/E1_DF_Mixed.html#print-means",
    "title": "E1_DF_Mixed",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(Accuracy$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_correct\nsem_correct\n\n\n\n\n500\n0.668\n0.013\n\n\n1000\n0.700\n0.012\n\n\n2000\n0.699\n0.012\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\nF\n0.698\n0.01\n\n\nR\n0.681\n0.01\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nexemplar\n0.588\n0.011\n\n\nnovel\n0.790\n0.009\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\n500\nF\n0.679\n0.018\n\n\n500\nR\n0.657\n0.018\n\n\n1000\nF\n0.719\n0.017\n\n\n1000\nR\n0.681\n0.018\n\n\n2000\nF\n0.694\n0.018\n\n\n2000\nR\n0.704\n0.018\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nexemplar\n0.551\n0.019\n\n\n500\nnovel\n0.785\n0.016\n\n\n1000\nexemplar\n0.619\n0.019\n\n\n1000\nnovel\n0.781\n0.016\n\n\n2000\nexemplar\n0.594\n0.019\n\n\n2000\nnovel\n0.804\n0.015\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nF\nexemplar\n0.603\n0.015\n\n\nF\nnovel\n0.792\n0.013\n\n\nR\nexemplar\n0.574\n0.015\n\n\nR\nnovel\n0.788\n0.013\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nF\nexemplar\n0.571\n0.027\n\n\n500\nF\nnovel\n0.788\n0.022\n\n\n500\nR\nexemplar\n0.532\n0.027\n\n\n500\nR\nnovel\n0.782\n0.022\n\n\n1000\nF\nexemplar\n0.644\n0.026\n\n\n1000\nF\nnovel\n0.794\n0.022\n\n\n1000\nR\nexemplar\n0.594\n0.027\n\n\n1000\nR\nnovel\n0.768\n0.023\n\n\n2000\nF\nexemplar\n0.594\n0.027\n\n\n2000\nF\nnovel\n0.794\n0.022\n\n\n2000\nR\nexemplar\n0.594\n0.027\n\n\n2000\nR\nnovel\n0.815\n0.021"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#comparisons",
    "href": "vignettes/E1_DF_Mixed.html#comparisons",
    "title": "E1_DF_Mixed",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nAccuracy$simple$DF_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nAccuracy$simple$test_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#write-up",
    "href": "vignettes/E1_DF_Mixed.html#write-up",
    "title": "E1_DF_Mixed",
    "section": "Write-up",
    "text": "Write-up\n\n## helper print functions\nqprint <- function(data,iv,level,dv){\n   data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv)\n}\n\nqprint_mean_sem <- function(data,iv,level,dv){\n   dv_mean <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[1])\n   \n   dv_sem <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[2])\n   \n   return(paste(\"M = \", \n    dv_mean,\n    \", SEM = \",\n    dv_sem,\n    sep=\"\"))\n   \n}\n\n# qprint(Accuracy$means,\"encoding_stimulus_time\",\"500\",\"mean_correct\")\n# qprint_mean_sem(Accuracy$means,\"encoding_stimulus_time\",\"500\",c(\"mean_correct\",\"sem_correct\"))\n\n# use data.table for interactions\n\n#t <- as.data.table(Accuracy$means$`encoding_stimulus_time:encoding_instruction`)\n#t[encoding_stimulus_time==500 & encoding_instruction == \"F\"]$mean_correct\n\nProportion correct for each subject in each condition was submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For completeness, each main effect and higher-order interaction is described in turn.\nThe main effect of encoding duration was not significant, \\(F(2, 66) = 2.62\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .080\\), \\(\\hat{\\eta}^2_G = .007\\). Proportion correct was similar across the 500 ms (M = 0.668, SEM = 0.013), 1000 ms (M = 0.7, SEM = 0.012), and 2000 ms (M = 0.699, SEM = 0.012) stimulus durations.\nThe main effect of encoding instruction was not significant, \\(F(1, 33) = 1.95\\), \\(\\mathit{MSE} = 0.01\\), \\(p = .171\\), \\(\\hat{\\eta}^2_G = .002\\). Proportion correct was similar for remember cues (M = 0.681, SEM = 0.01) and forget cues (M = 0.698, SEM = 0.01).\nThe main effect of lure type was significant, \\(F(1, 33) = 133.34\\), \\(\\mathit{MSE} = 0.03\\), \\(p < .001\\), \\(\\hat{\\eta}^2_G = .257\\). Proportion correct was higher for novel lures (M = 0.79, SEM = 0.009) than exemplar lures (M = 0.588, SEM = 0.011).\nThe main question of interest was whether directing forgetting would vary across the encoding duration times. The interaction between encoding instruction and encoding duration was, \\(F(2, 66) = 0.95\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .393\\), \\(\\hat{\\eta}^2_G = .003\\).\nPaired sample t-tests were used to assess the directed forgetting effect at each encoding duration. The directed forgetting effect is taken as the difference between proportion correct for remember minus forget items. At 500 ms, the directed forgetting effect was reversed and not significant, \\(M = -0.02\\), 95% CI \\([-0.07, 0.03]\\), \\(t(33) = -0.86\\), \\(p = .398\\). At 1000ms, the directed forgetting effect was reversed and not significant, \\(M = -0.04\\), 95% CI \\([-0.09, 0.01]\\), \\(t(33) = -1.61\\), \\(p = .118\\). And, at 2000 ms, the directed forgetting effect was again not detected, \\(M = 0.01\\), 95% CI \\([-0.03, 0.06]\\), \\(t(33) = 0.47\\), \\(p = .643\\).\nThe encoding duration by lure type interaction was, \\(F(2, 66) = 2.99\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .057\\), \\(\\hat{\\eta}^2_G = .008\\). In the 500 ms condition, proportion correct was higher for novel than exemplar lures, \\(M = 0.23\\), 95% CI \\([0.19, 0.28]\\), \\(t(33) = 10.00\\), \\(p < .001\\). The advantage for novel over exemplar items was smaller in the 1000 ms condition, \\(M = 0.16\\), 95% CI \\([0.11, 0.21]\\), \\(t(33) = 6.80\\), \\(p < .001\\), and 2000 ms condition \\(M = 0.21\\), 95% CI \\([0.16, 0.26]\\), \\(t(33) = 7.91\\), \\(p < .001\\).\nThe encoding instruction by lure type interaction was not significant, \\(F(1, 33) = 1.24\\), \\(\\mathit{MSE} = 0.01\\), \\(p = .273\\), \\(\\hat{\\eta}^2_G = .001\\). Similarly, the interaction between encoding duration, instruction, and lure type was not significant, \\(F(2, 66) = 0.01\\), \\(\\mathit{MSE} = 0.03\\), \\(p = .988\\), \\(\\hat{\\eta}^2_G = .000\\)."
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#conduct-analysis-1",
    "href": "vignettes/E1_DF_Mixed.html#conduct-analysis-1",
    "title": "E1_DF_Mixed",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nRT <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nRT$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE,\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt))\n\n# declare factors, IVS, subject variable, and DV\nRT$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nRT$factors$subject <- \"ID\"\nRT$factors$DV <- \"rt\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nRT$subject_means <- get_mean_sem(data=RT$filtered_data,\n                                       grouping_vars = c(RT$factors$subject,\n                                                         RT$factors$IVs),\n                                       dv = RT$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nRT$effects <- get_effect_names(RT$factors$IVs)\n\nRT$means <- lapply(RT$effects, FUN = function(x) {\n  get_mean_sem(data=RT$filtered_data,\n             grouping_vars = x,\n             dv = RT$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nRT$subject_means <- RT$subject_means %>%\n  mutate_at(RT$factors$IVs,factor) %>%\n  mutate_at(RT$factors$subject,factor)\n\n# run ANOVA\nRT$aov.out <- aov(mean_rt ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), RT$subject_means)\n\n# save printable summaries\nRT$apa_print <- papaja::apa_print(RT$aov.out)"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#graphs-1",
    "href": "vignettes/E1_DF_Mixed.html#graphs-1",
    "title": "E1_DF_Mixed",
    "section": "Graphs",
    "text": "Graphs\n\nRT$graphs$figure <- ggplot(RT$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_rt,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_rt-sem_rt,\n                    ymax = mean_rt+sem_rt),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(1000,2000))+\n  scale_y_continuous(breaks = seq(1000,2000,100))+\n  theme_classic(base_size=12)+\n  ylab(\"Mean RT (ms)\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E1: Mean RT by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nRT$graphs$figure"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#print-anova-1",
    "href": "vignettes/E1_DF_Mixed.html#print-anova-1",
    "title": "E1_DF_Mixed",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(RT$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n33\n42156245.885\n1277461.997\nNA\nNA\n\n\nencoding_stimulus_time\n2\n30341.855\n15170.928\n0.4795306\n0.6212139\n\n\nResiduals\n66\n2088044.555\n31637.039\nNA\nNA\n\n\nencoding_instruction\n1\n177573.172\n177573.172\n5.4367884\n0.0259695\n\n\nResiduals\n33\n1077826.505\n32661.409\nNA\nNA\n\n\ntest_condition\n1\n604114.759\n604114.759\n6.7383947\n0.0139775\n\n\nResiduals\n33\n2958536.560\n89652.623\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n2582.818\n1291.409\n0.0415036\n0.9593709\n\n\nResiduals\n66\n2053626.396\n31115.551\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n25851.951\n12925.976\n0.3841409\n0.6825481\n\n\nResiduals\n66\n2220837.224\n33649.049\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n57136.209\n57136.209\n1.2656738\n0.2686962\n\n\nResiduals\n33\n1489716.354\n45142.920\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n71921.183\n35960.592\n1.1484839\n0.3233724\n\n\nResiduals\n66\n2066549.737\n31311.360\nNA\nNA"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#print-means-1",
    "href": "vignettes/E1_DF_Mixed.html#print-means-1",
    "title": "E1_DF_Mixed",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(RT$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_rt\nsem_rt\n\n\n\n\n500\n1650.684\n18.405\n\n\n1000\n1650.887\n17.881\n\n\n2000\n1630.329\n17.658\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\nF\n1662.582\n15.328\n\n\nR\n1625.191\n13.987\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nexemplar\n1684.996\n15.225\n\n\nnovel\n1603.181\n14.070\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\n500\nF\n1668.729\n27.447\n\n\n500\nR\n1632.639\n24.530\n\n\n1000\nF\n1665.714\n25.728\n\n\n1000\nR\n1635.926\n24.835\n\n\n2000\nF\n1653.363\n26.477\n\n\n2000\nR\n1606.984\n23.309\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nexemplar\n1699.975\n26.751\n\n\n500\nnovel\n1601.832\n25.177\n\n\n1000\nexemplar\n1682.482\n26.421\n\n\n1000\nnovel\n1619.575\n24.079\n\n\n2000\nexemplar\n1672.553\n25.967\n\n\n2000\nnovel\n1588.167\n23.847\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nF\nexemplar\n1713.690\n22.923\n\n\nF\nnovel\n1611.474\n20.238\n\n\nR\nexemplar\n1655.898\n19.969\n\n\nR\nnovel\n1594.880\n19.559\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nF\nexemplar\n1717.554\n40.068\n\n\n500\nF\nnovel\n1620.339\n37.417\n\n\n500\nR\nexemplar\n1682.396\n35.490\n\n\n500\nR\nnovel\n1583.324\n33.723\n\n\n1000\nF\nexemplar\n1699.271\n38.618\n\n\n1000\nF\nnovel\n1632.158\n33.964\n\n\n1000\nR\nexemplar\n1665.387\n36.053\n\n\n1000\nR\nnovel\n1606.993\n34.178\n\n\n2000\nF\nexemplar\n1724.132\n40.490\n\n\n2000\nF\nnovel\n1581.961\n33.678\n\n\n2000\nR\nexemplar\n1619.728\n32.100\n\n\n2000\nR\nnovel\n1594.392\n33.818"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#comparisons-1",
    "href": "vignettes/E1_DF_Mixed.html#comparisons-1",
    "title": "E1_DF_Mixed",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nRT$simple$DF_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nRT$simple$test_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#write-up-1",
    "href": "vignettes/E1_DF_Mixed.html#write-up-1",
    "title": "E1_DF_Mixed",
    "section": "Write-up",
    "text": "Write-up\nMean reaction times on correct trials for each subject in each condition were submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For brevity we report only the significant effects. The full analysis is contained in supplementary materials.\nThe main effect of encoding instruction was significant, \\(F(1, 33) = 5.44\\), \\(\\mathit{MSE} = 32,661.41\\), \\(p = .026\\), \\(\\hat{\\eta}^2_G = .003\\). Mean reaction times were faster for choosing remember cued items (M = 1625.191, SEM = 13.987) than forget cued items (M = 1662.582, SEM = 15.328).\nThe main effect of lure type was significant, \\(F(1, 33) = 6.74\\), \\(\\mathit{MSE} = 89,652.62\\), \\(p = .014\\), \\(\\hat{\\eta}^2_G = .011\\). Mean reaction times were faster in the novel lure condition (M = 1603.181, SEM = 14.07) than exemplar lure condition (M = 1684.996, SEM = 15.225).\nThe remaining main effects and interactions were not significant."
  },
  {
    "objectID": "vignettes/E1_DF_Mixed.html#save-environment",
    "href": "vignettes/E1_DF_Mixed.html#save-environment",
    "title": "E1_DF_Mixed",
    "section": "save environment",
    "text": "save environment\n\nsave.image(\"data/E1/E1_data_write_up.RData\")"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html",
    "href": "vignettes/E2_DF_Blocked.html",
    "title": "E2_DF_Blocked",
    "section": "",
    "text": "Data collected 2/10/22"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#import-data",
    "href": "vignettes/E2_DF_Blocked.html#import-data",
    "title": "E2_DF_Blocked",
    "section": "Import Data",
    "text": "Import Data\n\n# Read the text file from JATOS ...\nread_file('data/E2/jatos_results_20220329195903.txt') %>%\n  # ... split it into lines ...\n  str_split('\\n') %>% first() %>%\n  # ... filter empty rows ...\n  discard(function(x) x == '') %>%\n  # ... parse JSON into a data.frame\n  map_dfr(fromJSON, flatten=T) -> all_data"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#demographics",
    "href": "vignettes/E2_DF_Blocked.html#demographics",
    "title": "E2_DF_Blocked",
    "section": "Demographics",
    "text": "Demographics\n\nlibrary(tidyr)\n\ndemographics <- all_data %>%\n  filter(trial_type == \"survey-html-form\") %>%\n  select(ID,response) %>%\n  unnest_wider(response) %>%\n  mutate(age = as.numeric(age))\n\nage_demographics <- demographics %>%\n  summarize(mean_age = mean(age),\n            sd_age = sd(age),\n            min_age = min(age),\n            max_age = max(age))\n\nfactor_demographics <- apply(demographics[-1], 2, table)\n\nA total of 45 participants were recruited from Amazon’s Mechanical Turk. Mean age was 37.9 (range = 25 to 65 ). There were 11 females, and 34 males. There were 42 right-handed participants, and NA left or both handed participants. 36 participants reported normal vision, and 8 participants reported corrected-to-normal vision. 41 participants reported English as a first language, and 4 participants reported English as a second language."
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#pre-processing",
    "href": "vignettes/E2_DF_Blocked.html#pre-processing",
    "title": "E2_DF_Blocked",
    "section": "Pre-processing",
    "text": "Pre-processing\nWe are interested in including participants who attempted to perform the task to the best of their ability. We adopted the following exclusion criteria.\n\nLower than 75% correct during the encoding task. This means that participants failed to correctly press the F or R keys on each trial.\n\n\n# select data from the study phase\nstudy_accuracy <- all_data %>%\n  filter(experiment_phase == \"study\",\n         is.na(correct) == FALSE) %>%\n  group_by(ID)%>%\n  summarize(mean_correct = mean(correct))\n\nstudy_excluded_subjects <- study_accuracy %>%\n  filter(mean_correct < .75) %>%\n  pull(ID)\n\nggplot(study_accuracy, aes(x=mean_correct))+\n  coord_cartesian(xlim=c(0,1))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean correct responses \\n for each subject during study phase\")\n\n\n\n\n\nMore than 25% Null responses (120*.25 = 30) during test. NULL responses mean that the participant did not respond on a test trial after 10 seconds.\n\n\n# select data from the study phase\ntest_null <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response ==\"NULL\") %>%\n  group_by(ID) %>%\n  count()\n\ntest_null_excluded <- test_null %>%\n  filter(n > (120*.25)) %>%\n  pull(ID)\n\nggplot(test_null, aes(x=n))+\n  geom_vline(xintercept=30)+\n  geom_histogram()+\n  ggtitle(\"Histogram of count of null responses \\n for each subject during test\")\n\n\n\n\n\nHigher than 75% response bias in the recognition task. This suggests that participants were simply pressing the same button on most trials.\n\n\ntest_response_bias <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\") %>%\n  mutate(response = as.numeric(response)) %>%\n  group_by(ID, response) %>%\n  count() %>%\n  pivot_wider(names_from = response,\n              values_from = n,\n              values_fill = 0) %>%\n  mutate(bias = abs(`0` - `1`)/120)\n\ntest_response_bias_excluded <- test_response_bias %>%\n  filter(bias > .75) %>%\n  pull(ID)\n\nggplot(test_response_bias, aes(x=bias))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nMaking responses too fast during the recognition memory test, indicating that they weren’t performing the task. We excluded participants whose mean RT was less than 300 ms.\n\n\ntest_mean_rt <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\",\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt)) %>%\n  group_by(ID) %>%\n  summarize(mean_RT = mean(rt))\n\ntest_mean_rt_excluded <- test_mean_rt %>%\n  filter(mean_RT < 300) %>%\n  pull(ID)\n\nggplot(test_mean_rt, aes(x=mean_RT))+\n  geom_vline(xintercept=300)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nSubjects are included if they perform better than 55% correct on the novel lures.\n\n\ntest_mean_novel_accuracy <- all_data %>%\n  filter(experiment_phase == \"test\",\n         test_condition == \"novel\") %>%\n  mutate(correct = as.logical(correct)) %>%\n  group_by(ID) %>%\n  summarize(mean_correct = mean(correct))\n\ntest_mean_novel_accuracy_excluded <- test_mean_novel_accuracy %>%\n  filter(mean_correct < .4) %>%\n  pull(ID)\n\nggplot(test_mean_novel_accuracy, aes(x=mean_correct))+\n  geom_vline(xintercept=.4)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean accuracy for novel lures \\n for each subject during test phase\")"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#all-exclusions",
    "href": "vignettes/E2_DF_Blocked.html#all-exclusions",
    "title": "E2_DF_Blocked",
    "section": "All exclusions",
    "text": "All exclusions\n\nall_excluded <- unique(c(study_excluded_subjects,\n                  test_null_excluded,\n                  test_response_bias_excluded,\n                  test_mean_rt_excluded,\n                  test_mean_novel_accuracy_excluded))\n\nlength(all_excluded)\n\n[1] 6\n\n\nOur participants were recruited online and completed the experiment from a web browser. Our experiment script requests that participants attempt the task to the best of their ability. Nevertheless, it is possible that participants complete the experiment and submit data without attempting to complete the task as directed. We developed a set of criteria to exclude participants whose performance indicated they were not attempting the task as instructed. These criteria also allowed us to confirm that the participants we included in the analysis did attempt the task as instructed to the best of their ability. We adopted the following five criteria:\nFirst, during the encoding phase participants responded to each instructional cue (to remember or forget the picture on each trial) by pressing “R” or “F” on the keyboard. This task demand further served as an attentional check. We excluded participants who scored lower than 75% correct on instructional cue identification responses. Second, participants who did not respond on more than 25% of trials in the recognition test were excluded. Third, we measured response bias (choosing the left or right picture) during the recognition test, and excluded participants who made 75% of their responses to one side (indicating they were repeatedly pressing the same button on each trial). Fourth, we excluded participants whose mean reaction time during the recognition test was less than 300ms, indicating they were pressing the buttons as fast as possible without making a recognition decision. Finally, we computed mean accuracy for the novel lure condition for all participants, and excluded participants whose mean accuracy was less than 55% for those items. All together 6 participants were excluded."
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#define-helper-functions",
    "href": "vignettes/E2_DF_Blocked.html#define-helper-functions",
    "title": "E2_DF_Blocked",
    "section": "Define Helper functions",
    "text": "Define Helper functions\nTo do, consider moving the functions into the R package for this project\n\n# attempt general solution\n\n## Declare helper functions\n\n################\n# get_mean_sem\n# data = a data frame\n# grouping_vars = a character vector of factors for analysis contained in data\n# dv = a string indicated the dependent variable colunmn name in data\n# returns data frame with grouping variables, and mean_{dv}, sem_{dv}\n# note: dv in mean_{dv} and sem_{dv} is renamed to the string in dv\n\nget_mean_sem <- function(data, grouping_vars, dv, digits=3){\n  a <- data %>%\n    group_by_at(grouping_vars) %>%\n    summarize(\"mean_{ dv }\" := round(mean(.data[[dv]]), digits),\n              \"sem_{ dv }\" := round(sd(.data[[dv]])/sqrt(length(.data[[dv]])),digits),\n              .groups=\"drop\")\n  return(a)\n}\n\n################\n# get_effect_names\n# grouping_vars = a character vector of factors for analysis\n# returns a named list\n# list contains all main effects and interaction terms\n# useful for iterating the computation means across design effects and interactions\n\nget_effect_names <- function(grouping_vars){\n  effect_names <- grouping_vars\n  if( length(grouping_vars > 1) ){\n    for( i in 2:length(grouping_vars) ){\n      effect_names <- c(effect_names,apply(combn(grouping_vars,i),2,paste0,collapse=\":\"))\n    }\n  }\n  effects <- strsplit(effect_names, split=\":\")\n  names(effects) <- effect_names\n  return(effects)\n}\n\n################\n# print_list_of_tables\n# table_list = a list of named tables\n# each table is printed \n# names are header level 3\n\nprint_list_of_tables <- function(table_list){\n  for(i in 1:length(table_list)){\n    cat(\"###\",names(table_list[i]))\n    cat(\"\\n\")\n    print(knitr::kable(table_list[[i]]))\n    cat(\"\\n\")\n  }\n}"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#conduct-analysis",
    "href": "vignettes/E2_DF_Blocked.html#conduct-analysis",
    "title": "E2_DF_Blocked",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nAccuracy <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nAccuracy$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE)\n\n# declare factors, IVS, subject variable, and DV\nAccuracy$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nAccuracy$factors$subject <- \"ID\"\nAccuracy$factors$DV <- \"correct\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nAccuracy$subject_means <- get_mean_sem(data=Accuracy$filtered_data,\n                                       grouping_vars = c(Accuracy$factors$subject,\n                                                         Accuracy$factors$IVs),\n                                       dv = Accuracy$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nAccuracy$effects <- get_effect_names(Accuracy$factors$IVs)\n\nAccuracy$means <- lapply(Accuracy$effects, FUN = function(x) {\n  get_mean_sem(data=Accuracy$filtered_data,\n             grouping_vars = x,\n             dv = Accuracy$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nAccuracy$subject_means <- Accuracy$subject_means %>%\n  mutate_at(Accuracy$factors$IVs,factor) %>%\n  mutate_at(Accuracy$factors$subject,factor)\n\n# run ANOVA\nAccuracy$aov.out <- aov(mean_correct ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), Accuracy$subject_means)\n\n# save printable summaries\nAccuracy$apa_print <- papaja::apa_print(Accuracy$aov.out)"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#graphs",
    "href": "vignettes/E2_DF_Blocked.html#graphs",
    "title": "E2_DF_Blocked",
    "section": "Graphs",
    "text": "Graphs\n\nAccuracy$graphs$figure <- ggplot(Accuracy$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_correct,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_correct-sem_correct,\n                    ymax = mean_correct+sem_correct),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(.4,1))+\n  geom_hline(yintercept=.5)+\n  scale_y_continuous(breaks = seq(0.4,1,.1))+\n  theme_classic(base_size=12)+\n  ylab(\"Proportion Correct\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E2: Proportion Correct by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nAccuracy$graphs$figure"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#print-anova",
    "href": "vignettes/E2_DF_Blocked.html#print-anova",
    "title": "E2_DF_Blocked",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(Accuracy$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n38\n6.4974786\n0.1709863\nNA\nNA\n\n\nencoding_stimulus_time\n2\n0.2638889\n0.1319444\n5.5368098\n0.0056915\n\n\nResiduals\n76\n1.8111111\n0.0238304\nNA\nNA\n\n\nencoding_instruction\n1\n0.0144444\n0.0144444\n0.6504279\n0.4249786\n\n\nResiduals\n38\n0.8438889\n0.0222076\nNA\nNA\n\n\ntest_condition\n1\n2.4700855\n2.4700855\n79.6634145\n0.0000000\n\n\nResiduals\n38\n1.1782479\n0.0310065\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n0.0338889\n0.0169444\n0.8266762\n0.4413962\n\n\nResiduals\n76\n1.5577778\n0.0204971\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n0.0105556\n0.0052778\n0.2553041\n0.7753426\n\n\nResiduals\n76\n1.5711111\n0.0206725\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n0.0218803\n0.0218803\n1.1289967\n0.2946956\n\n\nResiduals\n38\n0.7364530\n0.0193803\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n0.0846581\n0.0423291\n2.4242561\n0.0953629\n\n\nResiduals\n76\n1.3270085\n0.0174606\nNA\nNA"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#print-means",
    "href": "vignettes/E2_DF_Blocked.html#print-means",
    "title": "E2_DF_Blocked",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(Accuracy$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_correct\nsem_correct\n\n\n\n\n500\n0.610\n0.012\n\n\n1000\n0.632\n0.012\n\n\n2000\n0.667\n0.012\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\nF\n0.631\n0.01\n\n\nR\n0.642\n0.01\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nexemplar\n0.564\n0.010\n\n\nnovel\n0.709\n0.009\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\n500\nF\n0.595\n0.018\n\n\n500\nR\n0.624\n0.017\n\n\n1000\nF\n0.624\n0.017\n\n\n1000\nR\n0.640\n0.017\n\n\n2000\nF\n0.673\n0.017\n\n\n2000\nR\n0.662\n0.017\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nexemplar\n0.535\n0.018\n\n\n500\nnovel\n0.685\n0.017\n\n\n1000\nexemplar\n0.555\n0.018\n\n\n1000\nnovel\n0.709\n0.016\n\n\n2000\nexemplar\n0.601\n0.018\n\n\n2000\nnovel\n0.733\n0.016\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nF\nexemplar\n0.551\n0.015\n\n\nF\nnovel\n0.710\n0.013\n\n\nR\nexemplar\n0.576\n0.014\n\n\nR\nnovel\n0.708\n0.013\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nF\nexemplar\n0.528\n0.025\n\n\n500\nF\nnovel\n0.662\n0.024\n\n\n500\nR\nexemplar\n0.541\n0.025\n\n\n500\nR\nnovel\n0.708\n0.023\n\n\n1000\nF\nexemplar\n0.523\n0.025\n\n\n1000\nF\nnovel\n0.726\n0.023\n\n\n1000\nR\nexemplar\n0.587\n0.025\n\n\n1000\nR\nnovel\n0.692\n0.023\n\n\n2000\nF\nexemplar\n0.603\n0.025\n\n\n2000\nF\nnovel\n0.744\n0.022\n\n\n2000\nR\nexemplar\n0.600\n0.025\n\n\n2000\nR\nnovel\n0.723\n0.023"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#comparisons",
    "href": "vignettes/E2_DF_Blocked.html#comparisons",
    "title": "E2_DF_Blocked",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nAccuracy$simple$DF_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nAccuracy$simple$test_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#write-up",
    "href": "vignettes/E2_DF_Blocked.html#write-up",
    "title": "E2_DF_Blocked",
    "section": "Write-up",
    "text": "Write-up\n\n## helper print functions\nqprint <- function(data,iv,level,dv){\n   data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv)\n}\n\nqprint_mean_sem <- function(data,iv,level,dv){\n   dv_mean <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[1])\n   \n   dv_sem <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[2])\n   \n   return(paste(\"M = \", \n    dv_mean,\n    \", SEM = \",\n    dv_sem,\n    sep=\"\"))\n   \n}\n\n# qprint(Accuracy$means,\"encoding_stimulus_time\",\"500\",\"mean_correct\")\n# qprint_mean_sem(Accuracy$means,\"encoding_stimulus_time\",\"500\",c(\"mean_correct\",\"sem_correct\"))\n\n# use data.table for interactions\n\n#t <- as.data.table(Accuracy$means$`encoding_stimulus_time:encoding_instruction`)\n#t[encoding_stimulus_time==500 & encoding_instruction == \"F\"]$mean_correct\n\nProportion correct for each subject in each condition was submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For completeness, each main effect and higher-order interaction is described in turn.\nThe main effect of encoding duration was significant, \\(F(2, 76) = 5.54\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .006\\), \\(\\hat{\\eta}^2_G = .017\\). Proportion correct was lowest for the 500 ms duration (M = 0.61, SEM = 0.012), and higher for the 1000 ms (M = 0.632, SEM = 0.012), and 2000 ms (M = 0.667, SEM = 0.012) stimulus durations.\nThe main effect of encoding instruction was not significant, \\(F(1, 38) = 0.65\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .425\\), \\(\\hat{\\eta}^2_G = .001\\). Proportion correct was similar for remember cues (M = 0.642, SEM = 0.01) and forget cues (M = 0.631, SEM = 0.01).\nThe main effect of lure type was significant, \\(F(1, 38) = 79.66\\), \\(\\mathit{MSE} = 0.03\\), \\(p < .001\\), \\(\\hat{\\eta}^2_G = .137\\). Proportion correct was higher for novel lures (M = 0.709, SEM = 0.009) than exemplar lures (M = 0.564, SEM = 0.01).\nThe main question of interest was whether directing forgetting would vary across the encoding duration times. The interaction between encoding instruction and encoding duration was not significant, \\(F(2, 76) = 0.83\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .441\\), \\(\\hat{\\eta}^2_G = .002\\).\nPaired sample t-tests were used to assess the directed forgetting effect at each encoding duration. The directed forgetting effect is taken as the difference between proportion correct for remember minus forget items. At 500 ms, the directed forgetting effect was not significant, \\(M = 0.03\\), 95% CI \\([-0.01, 0.07]\\), \\(t(38) = 1.36\\), \\(p = .181\\). At 1000ms, the directed forgetting effect was not significant, \\(M = 0.02\\), 95% CI \\([-0.03, 0.06]\\), \\(t(38) = 0.63\\), \\(p = .531\\). And, at 2000 ms, the directed forgetting effect was again not detected, \\(M = -0.01\\), 95% CI \\([-0.06, 0.04]\\), \\(t(38) = -0.49\\), \\(p = .629\\).\nThe encoding duration by lure type interaction was not significnat, \\(F(2, 76) = 0.26\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .775\\), \\(\\hat{\\eta}^2_G = .001\\). The encoding instruction by lure type interaction was not significant, \\(F(1, 38) = 1.13\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .295\\), \\(\\hat{\\eta}^2_G = .001\\). Similarly, the interaction between encoding duration, instruction, and lure type was not significant, \\(F(2, 76) = 2.42\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .095\\), \\(\\hat{\\eta}^2_G = .005\\)."
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#conduct-analysis-1",
    "href": "vignettes/E2_DF_Blocked.html#conduct-analysis-1",
    "title": "E2_DF_Blocked",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nRT <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nRT$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE,\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt))\n\n# declare factors, IVS, subject variable, and DV\nRT$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nRT$factors$subject <- \"ID\"\nRT$factors$DV <- \"rt\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nRT$subject_means <- get_mean_sem(data=RT$filtered_data,\n                                       grouping_vars = c(RT$factors$subject,\n                                                         RT$factors$IVs),\n                                       dv = RT$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nRT$effects <- get_effect_names(RT$factors$IVs)\n\nRT$means <- lapply(RT$effects, FUN = function(x) {\n  get_mean_sem(data=RT$filtered_data,\n             grouping_vars = x,\n             dv = RT$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nRT$subject_means <- RT$subject_means %>%\n  mutate_at(RT$factors$IVs,factor) %>%\n  mutate_at(RT$factors$subject,factor)\n\n# run ANOVA\nRT$aov.out <- aov(mean_rt ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), RT$subject_means)\n\n# save printable summaries\nRT$apa_print <- papaja::apa_print(RT$aov.out)"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#graphs-1",
    "href": "vignettes/E2_DF_Blocked.html#graphs-1",
    "title": "E2_DF_Blocked",
    "section": "Graphs",
    "text": "Graphs\n\nRT$graphs$figure <- ggplot(RT$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_rt,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_rt-sem_rt,\n                    ymax = mean_rt+sem_rt),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(1000,2000))+\n  scale_y_continuous(breaks = seq(1000,2000,100))+\n  theme_classic(base_size=12)+\n  ylab(\"Mean RT (ms)\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E2: Mean RT by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nRT$graphs$figure"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#print-anova-1",
    "href": "vignettes/E2_DF_Blocked.html#print-anova-1",
    "title": "E2_DF_Blocked",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(RT$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n38\n5.837090e+07\n1536076.4179\nNA\nNA\n\n\nencoding_stimulus_time\n2\n7.792940e+04\n38964.7013\n1.0603772\n0.3513923\n\n\nResiduals\n76\n2.792702e+06\n36746.0764\nNA\nNA\n\n\nencoding_instruction\n1\n4.961696e+02\n496.1696\n0.0109629\n0.9171616\n\n\nResiduals\n38\n1.719847e+06\n45259.1256\nNA\nNA\n\n\ntest_condition\n1\n1.060501e+06\n1060500.6813\n7.3165610\n0.0101686\n\n\nResiduals\n38\n5.507919e+06\n144945.2390\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n1.463802e+05\n73190.0810\n2.7237547\n0.0720377\n\n\nResiduals\n76\n2.042198e+06\n26871.0254\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n1.788846e+04\n8944.2308\n0.2383893\n0.7884828\n\n\nResiduals\n76\n2.851476e+06\n37519.4245\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n1.626283e+04\n16262.8313\n0.4717691\n0.4963449\n\n\nResiduals\n38\n1.309937e+06\n34472.0134\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n2.239374e+04\n11196.8723\n0.3467641\n0.7080848\n\n\nResiduals\n76\n2.454009e+06\n32289.5985\nNA\nNA"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#print-means-1",
    "href": "vignettes/E2_DF_Blocked.html#print-means-1",
    "title": "E2_DF_Blocked",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(RT$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_rt\nsem_rt\n\n\n\n\n500\n1674.869\n17.745\n\n\n1000\n1694.952\n17.753\n\n\n2000\n1708.100\n18.705\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\nF\n1691.484\n14.709\n\n\nR\n1693.774\n14.805\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nexemplar\n1741.122\n15.939\n\n\nnovel\n1644.224\n13.401\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\n500\nF\n1666.543\n25.333\n\n\n500\nR\n1683.227\n24.867\n\n\n1000\nF\n1675.428\n24.909\n\n\n1000\nR\n1714.350\n25.297\n\n\n2000\nF\n1732.740\n26.145\n\n\n2000\nR\n1683.651\n26.740\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nexemplar\n1719.656\n26.787\n\n\n500\nnovel\n1630.139\n23.192\n\n\n1000\nexemplar\n1739.679\n27.266\n\n\n1000\nnovel\n1650.050\n22.626\n\n\n2000\nexemplar\n1764.156\n28.758\n\n\n2000\nnovel\n1652.480\n23.817\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nF\nexemplar\n1734.456\n22.578\n\n\nF\nnovel\n1648.512\n18.783\n\n\nR\nexemplar\n1747.777\n22.513\n\n\nR\nnovel\n1639.959\n19.124\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nF\nexemplar\n1703.950\n38.374\n\n\n500\nF\nnovel\n1629.232\n33.038\n\n\n500\nR\nexemplar\n1735.404\n37.414\n\n\n500\nR\nnovel\n1631.051\n32.599\n\n\n1000\nF\nexemplar\n1722.168\n38.455\n\n\n1000\nF\nnovel\n1628.687\n31.542\n\n\n1000\nR\nexemplar\n1757.009\n38.692\n\n\n1000\nR\nnovel\n1671.357\n32.451\n\n\n2000\nF\nexemplar\n1777.441\n40.468\n\n\n2000\nF\nnovel\n1687.923\n32.988\n\n\n2000\nR\nexemplar\n1750.837\n40.914\n\n\n2000\nR\nnovel\n1617.678\n34.287"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#comparisons-1",
    "href": "vignettes/E2_DF_Blocked.html#comparisons-1",
    "title": "E2_DF_Blocked",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nRT$simple$DF_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nRT$simple$test_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#write-up-1",
    "href": "vignettes/E2_DF_Blocked.html#write-up-1",
    "title": "E2_DF_Blocked",
    "section": "Write-up",
    "text": "Write-up\nMean reaction times on correct trials for each subject in each condition were submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For brevity we report only the significant effects. The full analysis is contained in supplementary materials.\nThe main effect of lure type was significant, \\(F(1, 38) = 7.32\\), \\(\\mathit{MSE} = 144,945.24\\), \\(p = .010\\), \\(\\hat{\\eta}^2_G = .014\\). Mean reaction times were faster in the novel lure condition (M = 1644.224, SEM = 13.401) than exemplar lure condition (M = 1741.122, SEM = 15.939).\nThe remaining main effects and interactions were not significant."
  },
  {
    "objectID": "vignettes/E2_DF_Blocked.html#save-environment",
    "href": "vignettes/E2_DF_Blocked.html#save-environment",
    "title": "E2_DF_Blocked",
    "section": "save environment",
    "text": "save environment\n\nsave.image(\"data/E2/E2_data_write_up.RData\")"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html",
    "href": "vignettes/E3_DF_pairs.html",
    "title": "E3_DF_paired",
    "section": "",
    "text": "Data collected 7/11/22"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#import-data",
    "href": "vignettes/E3_DF_pairs.html#import-data",
    "title": "E3_DF_paired",
    "section": "Import Data",
    "text": "Import Data\n\n# Read the text file from JATOS ...\n#read_file('data/E3/jatos_results_20221103143242.txt') %>%\nread_file('data/E3/jatos_results_20221212154552.txt') %>%\n  # ... split it into lines ...\n  str_split('\\n') %>% first() %>%\n  # ... filter empty rows ...\n  discard(function(x) x == '') %>%\n  # ... parse JSON into a data.frame\n  map_dfr(fromJSON, flatten=T) -> all_data"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#demographics",
    "href": "vignettes/E3_DF_pairs.html#demographics",
    "title": "E3_DF_paired",
    "section": "Demographics",
    "text": "Demographics\n\nlibrary(tidyr)\n\ndemographics <- all_data %>%\n  filter(trial_type == \"survey-html-form\") %>%\n  select(ID,response) %>%\n  unnest_wider(response) %>%\n  mutate(age = as.numeric(age))\n\nage_demographics <- demographics %>%\n  summarize(mean_age = mean(age),\n            sd_age = sd(age),\n            min_age = min(age),\n            max_age = max(age))\n\nfactor_demographics <- apply(demographics[-1], 2, table)\n\nA total of 26 participants were recruited from Amazon’s Mechanical Turk. Mean age was 19 (range = 18 to 22 ). There were 16 females, and 10 males. There were 24 right-handed participants, and NA left or both handed participants. 15 participants reported normal vision, and 8 participants reported corrected-to-normal vision. 22 participants reported English as a first language, and 4 participants reported English as a second language."
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#pre-processing",
    "href": "vignettes/E3_DF_pairs.html#pre-processing",
    "title": "E3_DF_paired",
    "section": "Pre-processing",
    "text": "Pre-processing\nWe are interested in including participants who attempted to perform the task to the best of their ability. We adopted the following exclusion criteria.\n\nLower than 75% correct during the encoding task. This means that participants failed to correctly press the F or R keys on each trial.\n\n\n# select data from the study phase\n# study_accuracy <- all_data %>%\n#   filter(experiment_phase == \"study\",\n#          is.na(correct) == FALSE) %>%\n#   group_by(ID)%>%\n#   summarize(mean_correct = mean(correct))\n\nstudy_accuracy <- all_data %>%\n  filter(experiment_phase == \"study\",\n         trial_type == \"html-slider-response\",\n         is.na(rt) == FALSE) %>%\n  mutate(response = as.numeric(unlist(response))) %>%\n  mutate(correct = case_when(encoding_instruction == \"R\" && response > 50 ~ TRUE,\n                             encoding_instruction == \"R\" && response < 50 ~ FALSE,\n                             encoding_instruction == \"F\" && response > 50 ~ FALSE,\n                             encoding_instruction == \"F\" && response < 50 ~ TRUE)) %>%\n  filter(is.na(correct) == FALSE) %>%\n  group_by(ID)%>%\n  summarize(mean_correct = mean(correct))\n\nstudy_excluded_subjects <- study_accuracy %>%\n  filter(mean_correct < .75) %>%\n  pull(ID)\n\nggplot(study_accuracy, aes(x=mean_correct))+\n  coord_cartesian(xlim=c(0,1))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean correct responses \\n for each subject during study phase\")\n\n\n\n\n\nMore than 25% Null responses (120*.25 = 30) during test. NULL responses mean that the participant did not respond on a test trial after 10 seconds.\n\n\n# select data from the study phase\ntest_null <- all_data %>%\n  filter(experiment_phase == \"test\"),\n         response ==\"NULL\") %>%\n  group_by(ID) %>%\n  count()\n\ntest_null_excluded <- test_null %>%\n  filter(n > (120*.25)) %>%\n  pull(ID)\n\nggplot(test_null, aes(x=n))+\n  geom_vline(xintercept=30)+\n  geom_histogram()+\n  ggtitle(\"Histogram of count of null responses \\n for each subject during test\")\n\n\nHigher than 75% response bias in the recognition task. This suggests that participants were simply pressing the same button on most trials.\n\n\ntest_response_bias <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\") %>%\n  mutate(response = as.numeric(response)) %>%\n  group_by(ID, response) %>%\n  count() %>%\n  pivot_wider(names_from = response,\n              values_from = n,\n              values_fill = 0) %>%\n  mutate(bias = abs(`0` - `1`)/120)\n\ntest_response_bias_excluded <- test_response_bias %>%\n  filter(bias > .75) %>%\n  pull(ID)\n\nggplot(test_response_bias, aes(x=bias))+\n  geom_vline(xintercept=.75)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nMaking responses too fast during the recognition memory test, indicating that they weren’t performing the task. We excluded participants whose mean RT was less than 300 ms.\n\n\ntest_mean_rt <- all_data %>%\n  filter(experiment_phase == \"test\",\n         response !=\"NULL\",\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt)) %>%\n  group_by(ID) %>%\n  summarize(mean_RT = mean(rt))\n\ntest_mean_rt_excluded <- test_mean_rt %>%\n  filter(mean_RT < 300) %>%\n  pull(ID)\n\nggplot(test_mean_rt, aes(x=mean_RT))+\n  geom_vline(xintercept=300)+\n  geom_histogram()+\n  ggtitle(\"Histogram of response bias \\n for each subject during test phase\")\n\n\n\n\n\nSubjects are included if they perform better than 55% correct on the novel lures.\n\n\ntest_mean_novel_accuracy <- all_data %>%\n  filter(experiment_phase == \"test\",\n         test_condition == \"novel\") %>%\n  mutate(correct = as.logical(correct)) %>%\n  group_by(ID) %>%\n  summarize(mean_correct = mean(correct))\n\ntest_mean_novel_accuracy_excluded <- test_mean_novel_accuracy %>%\n  filter(mean_correct < .55) %>%\n  pull(ID)\n\nggplot(test_mean_novel_accuracy, aes(x=mean_correct))+\n  geom_vline(xintercept=.55)+\n  geom_histogram()+\n  ggtitle(\"Histogram of mean accuracy for novel lures \\n for each subject during test phase\")"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#all-exclusions",
    "href": "vignettes/E3_DF_pairs.html#all-exclusions",
    "title": "E3_DF_paired",
    "section": "All exclusions",
    "text": "All exclusions\n\nall_excluded <- unique(c(study_excluded_subjects,\n                  test_response_bias_excluded,\n                  test_mean_rt_excluded,\n                  test_mean_novel_accuracy_excluded))\n\nlength(all_excluded)\n\n[1] 2\n\n\nOur participants were recruited online and completed the experiment from a web browser. Our experiment script requests that participants attempt the task to the best of their ability. Nevertheless, it is possible that participants complete the experiment and submit data without attempting to complete the task as directed. We developed a set of criteria to exclude participants whose performance indicated they were not attempting the task as instructed. These criteria also allowed us to confirm that the participants we included in the analysis did attempt the task as instructed to the best of their ability. We adopted the following five criteria:\nFirst, during the encoding phase participants responded to each instructional cue (to remember or forget the picture on each trial) by pressing “R” or “F” on the keyboard. This task demand further served as an attentional check. We excluded participants who scored lower than 75% correct on instructional cue identification responses. Second, participants who did not respond on more than 25% of trials in the recognition test were excluded. Third, we measured response bias (choosing the left or right picture) during the recognition test, and excluded participants who made 75% of their responses to one side (indicating they were repeatedly pressing the same button on each trial). Fourth, we excluded participants whose mean reaction time during the recognition test was less than 300ms, indicating they were pressing the buttons as fast as possible without making a recognition decision. Finally, we computed mean accuracy for the novel lure condition for all participants, and excluded participants whose mean accuracy was less than 55% for those items. All together 2 participants were excluded."
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#define-helper-functions",
    "href": "vignettes/E3_DF_pairs.html#define-helper-functions",
    "title": "E3_DF_paired",
    "section": "Define Helper functions",
    "text": "Define Helper functions\nTo do, consider moving the functions into the R package for this project\n\n# attempt general solution\n\n## Declare helper functions\n\n################\n# get_mean_sem\n# data = a data frame\n# grouping_vars = a character vector of factors for analysis contained in data\n# dv = a string indicated the dependent variable colunmn name in data\n# returns data frame with grouping variables, and mean_{dv}, sem_{dv}\n# note: dv in mean_{dv} and sem_{dv} is renamed to the string in dv\n\nget_mean_sem <- function(data, grouping_vars, dv, digits=3){\n  a <- data %>%\n    group_by_at(grouping_vars) %>%\n    summarize(\"mean_{ dv }\" := round(mean(.data[[dv]]), digits),\n              \"sem_{ dv }\" := round(sd(.data[[dv]])/sqrt(length(.data[[dv]])),digits),\n              .groups=\"drop\")\n  return(a)\n}\n\n################\n# get_effect_names\n# grouping_vars = a character vector of factors for analysis\n# returns a named list\n# list contains all main effects and interaction terms\n# useful for iterating the computation means across design effects and interactions\n\nget_effect_names <- function(grouping_vars){\n  effect_names <- grouping_vars\n  if( length(grouping_vars > 1) ){\n    for( i in 2:length(grouping_vars) ){\n      effect_names <- c(effect_names,apply(combn(grouping_vars,i),2,paste0,collapse=\":\"))\n    }\n  }\n  effects <- strsplit(effect_names, split=\":\")\n  names(effects) <- effect_names\n  return(effects)\n}\n\n################\n# print_list_of_tables\n# table_list = a list of named tables\n# each table is printed \n# names are header level 3\n\nprint_list_of_tables <- function(table_list){\n  for(i in 1:length(table_list)){\n    cat(\"###\",names(table_list[i]))\n    cat(\"\\n\")\n    print(knitr::kable(table_list[[i]]))\n    cat(\"\\n\")\n  }\n}"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#conduct-analysis",
    "href": "vignettes/E3_DF_pairs.html#conduct-analysis",
    "title": "E3_DF_paired",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nAccuracy <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nAccuracy$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE)\n\n# declare factors, IVS, subject variable, and DV\nAccuracy$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nAccuracy$factors$subject <- \"ID\"\nAccuracy$factors$DV <- \"correct\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nAccuracy$subject_means <- get_mean_sem(data=Accuracy$filtered_data,\n                                       grouping_vars = c(Accuracy$factors$subject,\n                                                         Accuracy$factors$IVs),\n                                       dv = Accuracy$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nAccuracy$effects <- get_effect_names(Accuracy$factors$IVs)\n\nAccuracy$means <- lapply(Accuracy$effects, FUN = function(x) {\n  get_mean_sem(data=Accuracy$filtered_data,\n             grouping_vars = x,\n             dv = Accuracy$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nAccuracy$subject_means <- Accuracy$subject_means %>%\n  mutate_at(Accuracy$factors$IVs,factor) %>%\n  mutate_at(Accuracy$factors$subject,factor)\n\n# run ANOVA\nAccuracy$aov.out <- aov(mean_correct ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), Accuracy$subject_means)\n\n# save printable summaries\nAccuracy$apa_print <- papaja::apa_print(Accuracy$aov.out)"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#graphs",
    "href": "vignettes/E3_DF_pairs.html#graphs",
    "title": "E3_DF_paired",
    "section": "Graphs",
    "text": "Graphs\n\nAccuracy$graphs$figure <- ggplot(Accuracy$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_correct,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_correct-sem_correct,\n                    ymax = mean_correct+sem_correct),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(.4,1))+\n  geom_hline(yintercept=.5)+\n  scale_y_continuous(breaks = seq(0.4,1,.1))+\n  theme_classic(base_size=12)+\n  ylab(\"Proportion Correct\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E2: Proportion Correct by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nAccuracy$graphs$figure"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#print-anova",
    "href": "vignettes/E3_DF_pairs.html#print-anova",
    "title": "E3_DF_paired",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(Accuracy$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n23\n3.8689475\n0.1682151\nNA\nNA\n\n\nencoding_stimulus_time\n2\n0.0012020\n0.0006010\n0.0273422\n0.9730440\n\n\nResiduals\n46\n1.0111165\n0.0219808\nNA\nNA\n\n\nencoding_instruction\n1\n0.0216320\n0.0216320\n1.3938997\n0.2498137\n\n\nResiduals\n23\n0.3569382\n0.0155191\nNA\nNA\n\n\ntest_condition\n1\n2.5511640\n2.5511640\n51.4151012\n0.0000003\n\n\nResiduals\n23\n1.1412362\n0.0496190\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n0.0229051\n0.0114526\n0.7442065\n0.4807442\n\n\nResiduals\n46\n0.7078927\n0.0153890\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n0.0928499\n0.0464250\n1.8154084\n0.1742398\n\n\nResiduals\n46\n1.1763459\n0.0255727\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n0.0197342\n0.0197342\n0.5849780\n0.4521445\n\n\nResiduals\n23\n0.7759046\n0.0337350\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n0.1357053\n0.0678526\n4.1481441\n0.0220657\n\n\nResiduals\n46\n0.7524379\n0.0163573\nNA\nNA"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#print-means",
    "href": "vignettes/E3_DF_pairs.html#print-means",
    "title": "E3_DF_paired",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(Accuracy$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_correct\nsem_correct\n\n\n\n\n500\n0.727\n0.014\n\n\n1000\n0.730\n0.014\n\n\n2000\n0.742\n0.014\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\nF\n0.729\n0.012\n\n\nR\n0.737\n0.012\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nexemplar\n0.640\n0.013\n\n\nnovel\n0.826\n0.010\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_correct\nsem_correct\n\n\n\n\n500\nF\n0.723\n0.02\n\n\n500\nR\n0.731\n0.02\n\n\n1000\nF\n0.738\n0.02\n\n\n1000\nR\n0.723\n0.02\n\n\n2000\nF\n0.727\n0.02\n\n\n2000\nR\n0.756\n0.02\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nexemplar\n0.646\n0.022\n\n\n500\nnovel\n0.808\n0.018\n\n\n1000\nexemplar\n0.648\n0.022\n\n\n1000\nnovel\n0.812\n0.018\n\n\n2000\nexemplar\n0.625\n0.022\n\n\n2000\nnovel\n0.858\n0.016\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\nF\nexemplar\n0.621\n0.018\n\n\nF\nnovel\n0.831\n0.014\n\n\nR\nexemplar\n0.657\n0.017\n\n\nR\nnovel\n0.821\n0.014\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_correct\nsem_correct\n\n\n\n\n500\nF\nexemplar\n0.626\n0.031\n\n\n500\nF\nnovel\n0.818\n0.025\n\n\n500\nR\nexemplar\n0.665\n0.030\n\n\n500\nR\nnovel\n0.798\n0.026\n\n\n1000\nF\nexemplar\n0.667\n0.031\n\n\n1000\nF\nnovel\n0.803\n0.025\n\n\n1000\nR\nexemplar\n0.631\n0.031\n\n\n1000\nR\nnovel\n0.823\n0.025\n\n\n2000\nF\nexemplar\n0.571\n0.033\n\n\n2000\nF\nnovel\n0.871\n0.021\n\n\n2000\nR\nexemplar\n0.675\n0.030\n\n\n2000\nR\nnovel\n0.844\n0.024"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#comparisons",
    "href": "vignettes/E3_DF_pairs.html#comparisons",
    "title": "E3_DF_paired",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nAccuracy$simple$DF_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$DF_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_correct) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nAccuracy$simple$test_500 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_1000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nAccuracy$simple$test_2000 <- Accuracy$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_correct = mean(mean_correct)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_correct) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#write-up",
    "href": "vignettes/E3_DF_pairs.html#write-up",
    "title": "E3_DF_paired",
    "section": "Write-up",
    "text": "Write-up\n\n## helper print functions\nqprint <- function(data,iv,level,dv){\n   data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv)\n}\n\nqprint_mean_sem <- function(data,iv,level,dv){\n   dv_mean <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[1])\n   \n   dv_sem <- data[[iv]] %>%\n   filter(.data[[iv]] == {level}) %>%\n   pull(dv[2])\n   \n   return(paste(\"M = \", \n    dv_mean,\n    \", SEM = \",\n    dv_sem,\n    sep=\"\"))\n   \n}\n\n# qprint(Accuracy$means,\"encoding_stimulus_time\",\"500\",\"mean_correct\")\n# qprint_mean_sem(Accuracy$means,\"encoding_stimulus_time\",\"500\",c(\"mean_correct\",\"sem_correct\"))\n\n# use data.table for interactions\n\n#t <- as.data.table(Accuracy$means$`encoding_stimulus_time:encoding_instruction`)\n#t[encoding_stimulus_time==500 & encoding_instruction == \"F\"]$mean_correct\n\nProportion correct for each subject in each condition was submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For completeness, each main effect and higher-order interaction is described in turn.\nThe main effect of encoding duration was significant, \\(F(2, 46) = 0.03\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .973\\), \\(\\hat{\\eta}^2_G = .000\\). Proportion correct was lowest for the 500 ms duration (M = 0.727, SEM = 0.014), and higher for the 1000 ms (M = 0.73, SEM = 0.014), and 2000 ms (M = 0.742, SEM = 0.014) stimulus durations.\nThe main effect of encoding instruction was not significant, \\(F(1, 23) = 1.39\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .250\\), \\(\\hat{\\eta}^2_G = .002\\). Proportion correct was similar for remember cues (M = 0.737, SEM = 0.012) and forget cues (M = 0.729, SEM = 0.012).\nThe main effect of lure type was significant, \\(F(1, 23) = 51.42\\), \\(\\mathit{MSE} = 0.05\\), \\(p < .001\\), \\(\\hat{\\eta}^2_G = .207\\). Proportion correct was higher for novel lures (M = 0.826, SEM = 0.01) than exemplar lures (M = 0.64, SEM = 0.013).\nThe main question of interest was whether directing forgetting would vary across the encoding duration times. The interaction between encoding instruction and encoding duration was not significant, \\(F(2, 46) = 0.74\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .481\\), \\(\\hat{\\eta}^2_G = .002\\).\nPaired sample t-tests were used to assess the directed forgetting effect at each encoding duration. The directed forgetting effect is taken as the difference between proportion correct for remember minus forget items. At 500 ms, the directed forgetting effect was not significant, \\(M = 0.02\\), 95% CI \\([-0.03, 0.07]\\), \\(t(23) = 0.87\\), \\(p = .392\\). At 1000ms, the directed forgetting effect was not significant, \\(M = -0.01\\), 95% CI \\([-0.06, 0.05]\\), \\(t(23) = -0.25\\), \\(p = .802\\). And, at 2000 ms, the directed forgetting effect was again not detected, \\(M = 0.04\\), 95% CI \\([-0.02, 0.09]\\), \\(t(23) = 1.41\\), \\(p = .173\\).\nThe encoding duration by lure type interaction was not significnat, \\(F(2, 46) = 1.82\\), \\(\\mathit{MSE} = 0.03\\), \\(p = .174\\), \\(\\hat{\\eta}^2_G = .009\\). The encoding instruction by lure type interaction was not significant, \\(F(1, 23) = 0.58\\), \\(\\mathit{MSE} = 0.03\\), \\(p = .452\\), \\(\\hat{\\eta}^2_G = .002\\). Similarly, the interaction between encoding duration, instruction, and lure type was not significant, \\(F(2, 46) = 4.15\\), \\(\\mathit{MSE} = 0.02\\), \\(p = .022\\), \\(\\hat{\\eta}^2_G = .014\\)."
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#conduct-analysis-1",
    "href": "vignettes/E3_DF_pairs.html#conduct-analysis-1",
    "title": "E3_DF_paired",
    "section": "Conduct Analysis",
    "text": "Conduct Analysis\n\n# create list to hold results\nRT <- list()\n\n# Pre-process data for analysis\n# assign to \"filtered_data\" object\nRT$filtered_data <- all_data %>%\n  filter(experiment_phase == \"test\", \n         ID %in% all_excluded == FALSE,\n         rt != \"NULL\") %>%\n  mutate(rt = as.numeric(rt)) %>%\n  filter(rt < 10000)\n\n# declare factors, IVS, subject variable, and DV\nRT$factors$IVs <- c(\"encoding_stimulus_time\",\n                          \"encoding_instruction\",\n                          \"test_condition\")\nRT$factors$subject <- \"ID\"\nRT$factors$DV <- \"rt\"\n\n## Subject-level means used for ANOVA\n# get individual subject means for each condition\nRT$subject_means <- get_mean_sem(data=RT$filtered_data,\n                                       grouping_vars = c(RT$factors$subject,\n                                                         RT$factors$IVs),\n                                       dv = RT$factors$DV)\n## Condition-level means\n# get all possible main effects and interactions\nRT$effects <- get_effect_names(RT$factors$IVs)\n\nRT$means <- lapply(RT$effects, FUN = function(x) {\n  get_mean_sem(data=RT$filtered_data,\n             grouping_vars = x,\n             dv = RT$factors$DV)\n})\n\n## ANOVA\n\n# ensure factors are factor class\nRT$subject_means <- RT$subject_means %>%\n  mutate_at(RT$factors$IVs,factor) %>%\n  mutate_at(RT$factors$subject,factor)\n\n# run ANOVA\nRT$aov.out <- aov(mean_rt ~ encoding_stimulus_time*encoding_instruction*test_condition + Error(ID/(encoding_stimulus_time*encoding_instruction*test_condition)), RT$subject_means)\n\n# save printable summaries\nRT$apa_print <- papaja::apa_print(RT$aov.out)"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#graphs-1",
    "href": "vignettes/E3_DF_pairs.html#graphs-1",
    "title": "E3_DF_paired",
    "section": "Graphs",
    "text": "Graphs\n\nRT$graphs$figure <- ggplot(RT$means$`encoding_stimulus_time:encoding_instruction:test_condition`, \n                                 aes(x=test_condition,\n                                     y=mean_rt,\n                                     group=encoding_instruction,\n                                     fill=encoding_instruction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_errorbar(aes(ymin = mean_rt-sem_rt,\n                    ymax = mean_rt+sem_rt),\n                width=.9, position=position_dodge2(width = 0.2, padding = 0.8))+\n  facet_wrap(~encoding_stimulus_time)+\n  coord_cartesian(ylim=c(1500,2500))+\n  scale_y_continuous(breaks = seq(1500,2500,100))+\n  theme_classic(base_size=12)+\n  ylab(\"Mean RT (ms)\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"E2: Mean RT by Stimulus Encoding Duration, \\n Encoding Instruction, and Lure Type\")\n\nRT$graphs$figure"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#print-anova-1",
    "href": "vignettes/E3_DF_pairs.html#print-anova-1",
    "title": "E3_DF_paired",
    "section": "Print ANOVA",
    "text": "Print ANOVA\nknitr::kable(xtable(summary(RT$aov.out)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nResiduals\n23\n85836700.351\n3732030.450\nNA\nNA\n\n\nencoding_stimulus_time\n2\n107013.567\n53506.783\n0.5007513\n0.6093405\n\n\nResiduals\n46\n4915238.115\n106853.003\nNA\nNA\n\n\nencoding_instruction\n1\n65432.948\n65432.948\n0.6048152\n0.4446732\n\n\nResiduals\n23\n2488293.434\n108186.671\nNA\nNA\n\n\ntest_condition\n1\n4304790.494\n4304790.494\n44.0665483\n0.0000009\n\n\nResiduals\n23\n2246833.145\n97688.398\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction\n2\n478689.763\n239344.882\n3.3366131\n0.0443468\n\n\nResiduals\n46\n3299712.685\n71732.884\nNA\nNA\n\n\nencoding_stimulus_time:test_condition\n2\n144780.645\n72390.323\n0.5911001\n0.5578684\n\n\nResiduals\n46\n5633487.591\n122467.122\nNA\nNA\n\n\nencoding_instruction:test_condition\n1\n34515.606\n34515.606\n0.2758887\n0.6044358\n\n\nResiduals\n23\n2877461.273\n125107.012\nNA\nNA\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n2\n8354.345\n4177.172\n0.0398563\n0.9609606\n\n\nResiduals\n46\n4821064.756\n104805.756\nNA\nNA"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#print-means-1",
    "href": "vignettes/E3_DF_pairs.html#print-means-1",
    "title": "E3_DF_paired",
    "section": "Print Means",
    "text": "Print Means\nprint_list_of_tables(RT$means)\n\nencoding_stimulus_time\n\n\n\nencoding_stimulus_time\nmean_rt\nsem_rt\n\n\n\n\n500\n1770.016\n37.079\n\n\n1000\n1732.204\n39.329\n\n\n2000\n1724.980\n34.979\n\n\n\n\n\nencoding_instruction\n\n\n\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\nF\n1745.990\n30.210\n\n\nR\n1738.778\n30.491\n\n\n\n\n\ntest_condition\n\n\n\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nexemplar\n1863.705\n32.250\n\n\nnovel\n1621.069\n27.958\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction\n\n\n\nencoding_stimulus_time\nencoding_instruction\nmean_rt\nsem_rt\n\n\n\n\n500\nF\n1829.174\n57.478\n\n\n500\nR\n1710.732\n46.737\n\n\n1000\nF\n1722.599\n53.963\n\n\n1000\nR\n1741.790\n57.274\n\n\n2000\nF\n1686.322\n44.605\n\n\n2000\nR\n1763.800\n53.918\n\n\n\n\n\nencoding_stimulus_time:test_condition\n\n\n\nencoding_stimulus_time\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nexemplar\n1862.007\n52.683\n\n\n500\nnovel\n1677.831\n51.902\n\n\n1000\nexemplar\n1859.860\n61.981\n\n\n1000\nnovel\n1605.350\n47.891\n\n\n2000\nexemplar\n1869.228\n52.545\n\n\n2000\nnovel\n1580.124\n45.247\n\n\n\n\n\nencoding_instruction:test_condition\n\n\n\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\nF\nexemplar\n1871.436\n46.934\n\n\nF\nnovel\n1627.542\n38.065\n\n\nR\nexemplar\n1856.395\n44.384\n\n\nR\nnovel\n1614.203\n41.125\n\n\n\n\n\nencoding_stimulus_time:encoding_instruction:test_condition\n\n\n\n\n\n\n\n\n\n\nencoding_stimulus_time\nencoding_instruction\ntest_condition\nmean_rt\nsem_rt\n\n\n\n\n500\nF\nexemplar\n1903.029\n79.750\n\n\n500\nF\nnovel\n1756.550\n82.634\n\n\n500\nR\nexemplar\n1821.668\n69.117\n\n\n500\nR\nnovel\n1597.436\n62.064\n\n\n1000\nF\nexemplar\n1870.850\n92.814\n\n\n1000\nF\nnovel\n1586.303\n57.558\n\n\n1000\nR\nexemplar\n1849.715\n83.044\n\n\n1000\nR\nnovel\n1625.888\n77.922\n\n\n2000\nF\nexemplar\n1839.599\n70.310\n\n\n2000\nF\nnovel\n1543.595\n54.569\n\n\n2000\nR\nexemplar\n1896.818\n77.623\n\n\n2000\nR\nnovel\n1619.698\n73.463"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#comparisons-1",
    "href": "vignettes/E3_DF_pairs.html#comparisons-1",
    "title": "E3_DF_paired",
    "section": "Comparisons",
    "text": "Comparisons\n\n## Encoding time x instruction\nRT$simple$DF_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$DF_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,encoding_instruction) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = encoding_instruction,\n              values_from = mean_rt) %>%\n  mutate(difference = R-F) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\n# encoding time x test condition\n\nRT$simple$test_500 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"500\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_1000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"1000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()\n\nRT$simple$test_2000 <- RT$subject_means %>%\n  filter(encoding_stimulus_time == \"2000\") %>%\n  group_by(ID,test_condition) %>%\n  summarize(mean_rt = mean(mean_rt)) %>%\n  pivot_wider(names_from = test_condition,\n              values_from = mean_rt) %>%\n  mutate(difference = novel-exemplar) %>%\n  pull(difference) %>%\n  t.test() %>%\n  papaja::apa_print()"
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#write-up-1",
    "href": "vignettes/E3_DF_pairs.html#write-up-1",
    "title": "E3_DF_paired",
    "section": "Write-up",
    "text": "Write-up\nMean reaction times on correct trials for each subject in each condition were submitted to a 3 (Encoding Duration: 500ms, 1000ms, 2000ms) x 2 (Encoding Instruction: Forget vs. Remember) x 2 (Lure type: Novel vs. Exemplar) fully repeated measures ANOVA. For brevity we report only the significant effects. The full analysis is contained in supplementary materials.\nThe main effect of lure type was significant, \\(F(1, 23) = 44.07\\), \\(\\mathit{MSE} = 97,688.40\\), \\(p < .001\\), \\(\\hat{\\eta}^2_G = .037\\). Mean reaction times were faster in the novel lure condition (M = 1621.069, SEM = 27.958) than exemplar lure condition (M = 1863.705, SEM = 32.25).\nThe remaining main effects and interactions were not significant."
  },
  {
    "objectID": "vignettes/E3_DF_pairs.html#save-environment",
    "href": "vignettes/E3_DF_pairs.html#save-environment",
    "title": "E3_DF_paired",
    "section": "save environment",
    "text": "save environment\n\nsave.image(\"data/E3/E3_data_write_up.RData\")"
  },
  {
    "objectID": "vignettes/Experiments.html",
    "href": "vignettes/Experiments.html",
    "title": "Experiments",
    "section": "",
    "text": "This project currently involves two experiments. The design and motivation for each are described in the honors thesis manuscript.\nIf you are interested in participating in the experiments, then you can try them out here. There is no compensation for participation except for our appreciation that you have contributed to our research project. If you agree to participate and you complete the experiment, then your data will added to the public data repository in this project. All data that is collected is completely anonymous.\nWe plan to leave these links here on a permanent basis. This will allow us to assess how the results stabilize across large groups of participants."
  },
  {
    "objectID": "vignettes/Experiments.html#experiment-1-mixed-stimulus-duration",
    "href": "vignettes/Experiments.html#experiment-1-mixed-stimulus-duration",
    "title": "Experiments",
    "section": "Experiment 1: Mixed stimulus duration",
    "text": "Experiment 1: Mixed stimulus duration\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated time:\n15-20 minutes\n\n\n\n\nThis is a directed forgetting task for pictures of natural scenes. You will be shown many pictures and asked to remember or forget each picture. Then you will be given a memory test.\n\n\nFirst Time?\nClick this link to participate in the experiment\nTry Experiment (once only)\n\n\nTrying again?\nWe are interested in data from first-time attempts, so please click the first link (which you can do only once). But, if you want to try again, go for it here:\nTry Experiment Again"
  },
  {
    "objectID": "vignettes/Experiments.html#experiment-2-blocked-stimulus-duration",
    "href": "vignettes/Experiments.html#experiment-2-blocked-stimulus-duration",
    "title": "Experiments",
    "section": "Experiment 2: Blocked stimulus duration",
    "text": "Experiment 2: Blocked stimulus duration\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated time:\n15-20 minutes\n\n\n\n\nThis is a directed forgetting task for pictures of natural scenes. You will be shown many pictures and asked to remember or forget each picture. Then you will be given a memory test.\n\n\nFirst Time?\nClick this link to participate in the experiment\nTry Experiment (once only)\n\n\nTrying again?\nWe are interested in data from first-time attempts, so please click the first link (which you can do only once). But, if you want to try again, go for it here:\nTry Experiment Again"
  },
  {
    "objectID": "vignettes/power_analysis.html",
    "href": "vignettes/power_analysis.html",
    "title": "power_analysis",
    "section": "",
    "text": "Conduct a simulation-based power analysis."
  },
  {
    "objectID": "vignettes/power_analysis.html#set-predictions",
    "href": "vignettes/power_analysis.html#set-predictions",
    "title": "power_analysis",
    "section": "Set predictions",
    "text": "Set predictions\nWe replicated Experiment 1 of Ahmad, Tan, and Hockley (2019), with the addition of a stimulus duration manipulation. The original study presented all pictures for 2000ms during the encoding phase.\nWe presented pictures for 500ms, 1000ms, or 2000ms during the encoding phase. We assumed that reducing encoding would generally reduce the memorability of pictures. We programmed a main effect of stimulus duration into the prediction vector, and assumed that proportion correct would decrease by 5% for each shorter duration. We also assumed that directed forgetting would be easier for less memorable stimuli, so we increased the directed forgetting effect for exemplar lures by 5% for each shorter duration. We also programmed an advantage for novel over exemplar lures.\n\nlibrary(tidyverse)\n\n# save results to this list\npower_analysis <- list()\n\n# predictions for mean accuracy in each condition (see paper for rationale)\npower_analysis$predictions = c(.90-.1, #R 500 Novel\n                .80-.1, #R 500 Exemplar\n                .90-.05, #R 1000 Novel\n                .80-.05, #R 1000 Exemplar\n                .90, #R 2000 Novel\n                .80, #R 2000 Exemplar\n                .90-.1, #F 500 Novel\n                .70-.1-.1, #F 500 Exemplar\n                .90-.05, #F 1000 Novel\n                .70-.05-.05, #F 1000 Exemplar\n                .90, #F 2000 Novel\n                .70 #F 2000 Exemplar\n)"
  },
  {
    "objectID": "vignettes/power_analysis.html#graph-predictions",
    "href": "vignettes/power_analysis.html#graph-predictions",
    "title": "power_analysis",
    "section": "Graph Predictions",
    "text": "Graph Predictions\n\n\n# graph\npower_analysis$predicted_means <- data.frame(\n  encoding_cue = rep(c(\"R\",\"F\"), each = 6),\n  image_duration = rep(rep(c(.5,1,2), each = 2),2),\n  test_condition = rep(rep(c(\"Novel\",\"Exemplar\"),each = 1),6),\n  accuracy = power_analysis$predictions\n)\n\npower_analysis$prediction_graph <- ggplot(\n  power_analysis$predicted_means,\n  aes(x=test_condition,y = accuracy, fill=encoding_cue))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_hline(yintercept=.5)+\n  coord_cartesian(ylim=c(.4,1))+\n  scale_y_continuous(breaks = seq(0.4,1,.1))+\n  facet_wrap(~image_duration)+\n  ylab(\"Proportion Correct\")+\n  xlab(\"Lure Type\")+\n  scale_fill_discrete(name = \" Encoding \\n Instruction\") +\n  ggtitle(\"Predictions for mean accuracy \\n as a function of Stimulus Duration, \\n Encoding Instruction, and Lure Type\")\n\npower_analysis$prediction_graph\n\n\n\n\n\n\n# function to generate simulated data for one subject\n# we use the binomial to generate decisions for old/new on each trial\n# the binomial probability of success is set to the mean accuracy for each condition\n# we simulate all 120 trials for a single simulated subject.\ncreate_n_subjects <- function(n){\n  \n  all_data <- data.frame()\n  for(i in 1:n){\n    subject_data <- data.frame(\n      sub_num = i,\n      encoding_cue = rep(c(\"R\",\"F\"), each = 60),\n      image_duration = rep(rep(c(.5,1,2), each = 20),2),\n      test_condition = rep(rep(c(\"Novel\",\"Exemplar\"),each = 10),6),\n      accuracy = rbinom(120,1,rep(power_analysis$predictions,each=10))\n      )\n    \n    all_data <- rbind(all_data,subject_data)\n  }\n  \n  return(all_data)\n  \n}"
  },
  {
    "objectID": "vignettes/power_analysis.html#run-simulation",
    "href": "vignettes/power_analysis.html#run-simulation",
    "title": "power_analysis",
    "section": "Run simulation",
    "text": "Run simulation\nWe run a simulation of the power of our design to detect effects of interest with n = 35.\n\n\n\npower_analysis$all_sim_data <- data.frame()\nnum_subs <- 35\nnum_sims <- 100\nfor(i in 1:num_sims){\n  #print(i)\n  simulated_data <- create_n_subjects(n=num_subs)\n  \n  # ensure factors are factor class\n  simulated_data <- simulated_data %>%\n    mutate_at(c(\"sub_num\",\"encoding_cue\",\"image_duration\",\"test_condition\"), factor) \n  \n  ANOVA_means <- simulated_data %>%\n    group_by(sub_num,encoding_cue,image_duration,test_condition) %>%\n    summarize(mean_accuracy =  mean(accuracy), .groups=\"drop\")\n  \n  sim_anova <- aov(mean_accuracy ~ encoding_cue*image_duration*test_condition + Error(sub_num/(encoding_cue*image_duration*test_condition)), data = ANOVA_means)\n  save_summary <- summary(sim_anova)\n  \n  cue <- save_summary$`Error: sub_num:encoding_cue`[[1]]$`Pr(>F)`[1]\n  cue_duration <- save_summary$`Error: sub_num:encoding_cue:image_duration`[[1]]$`Pr(>F)`[1]\n  cue_test <- save_summary$`Error: sub_num:encoding_cue:test_condition`[[1]]$`Pr(>F)`[1]\n  cue_duration_test <- save_summary$`Error: sub_num:encoding_cue:image_duration:test_condition`[[1]]$`Pr(>F)`[1]\n  \n  t_df <- data.frame(cue,\n                     cue_duration,\n                     cue_test,\n                     cue_duration_test)\n  power_analysis$all_sim_data <- rbind(power_analysis$all_sim_data,t_df)\n}\n\n\nproportion_sig <- function(ps){\n  length(ps[ps<.05])/length(ps)\n}\n\n\n# proportion significant for relevant effects\npower_analysis$sim_power <- apply(power_analysis$all_sim_data,2,proportion_sig)\npower_analysis$sim_power\n#>               cue      cue_duration          cue_test cue_duration_test \n#>              1.00              0.24              1.00              0.26"
  },
  {
    "objectID": "vignettes/power_analysis.html#save-out",
    "href": "vignettes/power_analysis.html#save-out",
    "title": "power_analysis",
    "section": "save out",
    "text": "save out\n\nsave.image(\"data/power.RData\")"
  }
]